# Ollama Provider Configuration Example  
# Copy this file to .env.ollama for local LLM usage

LLM_PROVIDER=ollama
MODEL_NAME=ollama:qwen3:32b

# Common configuration (used by all providers)
API_KEY=ollama
BASE_URL=http://localhost:11434
MODEL=qwen3:32b
MAX_TOKENS=4096
TEMPERATURE=0.1

# Provider-specific fallback (for compatibility)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama-specific optimizations
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=3
OLLAMA_KEEP_ALIVE=-1